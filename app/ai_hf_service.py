import os
import aiohttp
import logging
import asyncio
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)


class CodeExplainerAI:
    def __init__(self):
        self.api_key = os.getenv("HF_API_KEY", "")
        self.is_production = os.getenv("RENDER", False)  # Render sets this
        self.model_cache = {}

    async def explain_code(self, code: str, language: str = "auto") -> str:
        """–ü—Ä–æ–¥–∞–∫—à–µ–Ω –≤–µ—Ä—Å–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""

        if not code.strip():
            return "‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"

        # –í—Å–µ–≥–¥–∞ –±—ã—Å—Ç—Ä—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        local_result = self._local_analysis(code, language)

        if not self.api_key:
            return local_result

        # AI –∞–Ω–∞–ª–∏–∑ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
        try:
            ai_result = await asyncio.wait_for(
                self._production_ai_analysis(code, language),
                timeout=25.0  # –¢–∞–π–º–∞—É—Ç 25 —Å–µ–∫—É–Ω–¥
            )

            if self._is_valid_ai_response(ai_result):
                return f"ü§ñ **AI –ê–Ω–∞–ª–∏–∑:**\n\n{ai_result}\n\n---\nüîç **–î–µ—Ç–∞–ª–∏:**\n{local_result}"
            else:
                return local_result + "\n\n‚ö†Ô∏è AI –Ω–µ —Å–º–æ–≥ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∫–æ–¥"

        except asyncio.TimeoutError:
            logger.warning("AI –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–≤—ã—Å–∏–ª –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è")
            return local_result + "\n\n‚è≥ AI –∑–∞–Ω—è—Ç, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É"
        except Exception as e:
            logger.error(f"AI –æ—à–∏–±–∫–∞: {e}")
            return local_result + f"\n\nüîß –í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ AI"

    async def _production_ai_analysis(self, code: str, language: str) -> str:
        """–ü—Ä–æ–¥–∞–∫—à–µ–Ω –≤–µ—Ä—Å–∏—è AI –∞–Ω–∞–ª–∏–∑–∞"""

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        strategies = [
            self._try_code_llama,
            self._try_dialogpt,
            self._try_gpt2
        ]

        for strategy in strategies:
            try:
                result = await strategy(code, language)
                if self._is_valid_ai_response(result):
                    return result
            except Exception as e:
                logger.info(f"–°—Ç—Ä–∞—Ç–µ–≥–∏—è {strategy.__name__} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {e}")
                continue

        return ""

    async def _try_code_llama(self, code: str, language: str) -> str:
        """CodeLlama - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞"""
        url = "https://api-inference.huggingface.co/models/codellama/CodeLlama-7b-Instruct-hf"

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        prompt = f"[INST] –û–±—ä—è—Å–Ω–∏ —ç—Ç–æ—Ç {language} –∫–æ–¥:\n\n```{language}\n{code}\n```\n\n–û–±—ä—è—Å–Ω–∏ —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∫–æ–¥, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –¥–ª—è —á–µ–≥–æ –æ–Ω –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω. [/INST]"

        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 800,
                "temperature": 0.3,
                "do_sample": True
            }
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload, headers=headers, timeout=30) as response:
                if response.status == 200:
                    result = await response.json()
                    return self._extract_text(result)
                elif response.status == 503:
                    # –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è - –∂–¥–µ–º
                    await asyncio.sleep(15)
                    return await self._try_code_llama(code, language)
                else:
                    return ""

    async def _try_dialogpt(self, code: str, language: str) -> str:
        """DialoGPT - –Ω–∞–¥–µ–∂–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –¥–∏–∞–ª–æ–≥–∞"""
        url = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        prompt = f"–û–±—ä—è—Å–Ω–∏ —ç—Ç–æ—Ç {language} –∫–æ–¥ –ø–æ–¥—Ä–æ–±–Ω–æ: {code[:600]}"

        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 500,
                "temperature": 0.7,
                "do_sample": True
            }
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload, headers=headers, timeout=20) as response:
                if response.status == 200:
                    result = await response.json()
                    return self._extract_text(result)
                return ""

    async def _try_gpt2(self, code: str, language: str) -> str:
        """GPT2 - –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å"""
        url = "https://api-inference.huggingface.co/models/gpt2"

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        prompt = f"Explain this {language} code: {code[:400]}"

        payload = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 300,
                "temperature": 0.7
            }
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload, headers=headers, timeout=15) as response:
                if response.status == 200:
                    result = await response.json()
                    return self._extract_text(result)
                return ""

    def _extract_text(self, result) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ HF"""
        if isinstance(result, list) and len(result) > 0:
            if 'generated_text' in result[0]:
                text = result[0]['generated_text']
                if '–û–±—ä—è—Å–Ω–∏ —ç—Ç–æ—Ç' in text:
                    parts = text.split('–û–±—ä—è—Å–Ω–∏ —ç—Ç–æ—Ç')
                    if len(parts) > 1:
                        text = parts[1].strip()
                return text
        return ""

    def _is_valid_ai_response(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –æ—Ç–≤–µ—Ç AI –≤–∞–ª–∏–¥–Ω—ã–π"""
        if not text or text == "null":
            return False
        if len(text.strip()) < 30:
            return False
        if "error" in text.lower() or "exception" in text.lower():
            return False
        return True

    def _local_analysis(self, code: str, language: str) -> str:
        """–õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–æ—Å—Ç–∞–≤–ª—è–µ–º –≤–∞—à —Ç–µ–∫—É—â–∏–π)"""
        # ... –≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ ...
        return f"–õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: {len(code)} —Å–∏–º–≤–æ–ª–æ–≤, {language}"
